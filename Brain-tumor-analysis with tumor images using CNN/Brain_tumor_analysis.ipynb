{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from functools import reduce\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Input,Flatten,Lambda,Dense,BatchNormalization\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Nadam"
      ],
      "metadata": {
        "id": "3ZWqkgWenVOO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_folder = '/content/drive/MyDrive/datasets/Brain MRI'\n",
        "print(os.path.exists(dir_folder))\n",
        "# yes_dir = os.path.join(dir_folder,'yes')\n",
        "# no_dir = os.path.join(dir_folder,'no')\n",
        "# def extract_files(base1,base2):\n",
        "#   yes_files = []\n",
        "#   no_files = []\n",
        "#   for i,j in zip(os.listdir(base1),os.listdir(base2)):\n",
        "#       yes_full_path = os.path.join(base1,i)\n",
        "#       no_full_path = os.path.join(base2,j)\n",
        "#       yes_files.append(yes_full_path)\n",
        "#       no_files.append(no_full_path)\n",
        "#   return yes_files,no_files\n",
        "# yes_files,no_files = extract_files(yes_dir,no_dir)"
      ],
      "metadata": {
        "id": "MgLfNRcwfaUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d12d8e2-f800-4393-b9f7-369523bdc698"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_image(files,label_value):\n",
        "#   images = []\n",
        "#   labels = [label_value] * len(files)\n",
        "#   for file in yes_files:\n",
        "#       img = Image.open(file)\n",
        "#       img = img.convert('L')\n",
        "#       img = img.resize((224, 224))\n",
        "#       img_array = np.array(img)\n",
        "#       images.append(img_array)\n",
        "\n",
        "#   result = np.stack(images, axis=0)\n",
        "\n",
        "#   return result,labels\n",
        "\n",
        "# # no_images,labels = load_image(no_files,0)\n",
        "# yes_images,labels = load_image(yes_files,1)"
      ],
      "metadata": {
        "id": "pcqj2C9Zf352"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_gen = ImageDataGenerator(rescale=1./255,\n",
        "                               validation_split=0.2,\n",
        "                               rotation_range=40,\n",
        "                               height_shift_range=0.2,\n",
        "                               width_shift_range=0.2,\n",
        "                               shear_range=0.2,\n",
        "                               zoom_range=0.2,\n",
        "                               fill_mode='nearest',\n",
        "                               horizontal_flip=True)"
      ],
      "metadata": {
        "id": "pkibDAPYf30X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_gen.flow_from_directory(dir_folder,\n",
        "                                                target_size=(224,224),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='categorical',\n",
        "                                                subset='training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acplYzhaf3xx",
        "outputId": "9c426dfc-62c5-4f97-bbef-6fa5e7f480e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 47 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gray_scale_layer = Lambda(lambda x: tf.image.rgb_to_grayscale(x))\n",
        "\n",
        "model = Sequential([\n",
        "    gray_scale_layer,\n",
        "    Conv2D(32,(3,3),activation='relu',input_shape=(224,224,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64,(3,3),activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128,(3,3),activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(256,(3,3),activation='relu',),\n",
        "    MaxPooling2D((2,2)),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dense(512,activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(256,activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(2,activation='softmax')\n",
        "])\n",
        "input_shape = Input(shape=(224,224,3))\n",
        "Model(inputs=input_shape,outputs=model(input_shape))\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Nadam(learning_rate=0.01),metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True)\n",
        "\n",
        "epochs=20\n",
        "model.fit(train_generator,\n",
        "          callbacks=[early_stopping],\n",
        "          steps_per_epoch=len(train_generator) // 32,\n",
        "          epochs=epochs,\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCS05hE2f3vE",
        "outputId": "0b48d720-8e97-4f87-80e9-00f13d8f992b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7s/step - accuracy: 0.6619 - loss: 1.3760\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5988 - loss: 2.7863 \n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.4512 - loss: 2.7734 \n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.6339 - loss: 0.9645 \n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.7957 - loss: 0.5598 \n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.8057 - loss: 0.5331\n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8199 - loss: 0.3776\n",
            "Epoch 8/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.8382 - loss: 0.4199\n",
            "Epoch 9/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.8421 - loss: 0.4162\n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8785 - loss: 0.3074\n",
            "Epoch 11/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.8941 - loss: 0.4036 \n",
            "Epoch 12/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9210 - loss: 0.2625\n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9433 - loss: 0.2759\n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8836 - loss: 0.2759 \n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.8978 - loss: 0.2617 \n",
            "Epoch 16/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9352 - loss: 0.1568\n",
            "Epoch 17/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9366 - loss: 0.1616\n",
            "Epoch 18/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8704 - loss: 0.3195\n",
            "Epoch 19/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9352 - loss: 0.1894\n",
            "Epoch 20/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8978 - loss: 0.2027 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a4da3f07f40>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {\n",
        "    0:'no',\n",
        "    1:'yes'\n",
        "}"
      ],
      "metadata": {
        "id": "-QCp0OPxwYfZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(file):\n",
        "  img = Image.open(file)\n",
        "  img = img.resize((224, 224))\n",
        "  # img = img.convert('L')\n",
        "  img = img.convert('RGB')  # Convert to RGB to match model input\n",
        "  img = np.array(img)\n",
        "  # Add a batch dimension\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  # Preprocess using the same function used for training\n",
        "  # Rescale the image\n",
        "  img = img / 255.0\n",
        "  prediction = model.predict(img)\n",
        "  return classes[np.argmax(prediction)]\n",
        "\n",
        "file = '/content/drive/MyDrive/datasets/Brain MRI/yes/Y10.jpg'\n",
        "predict(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MZwv4hCJpr1B",
        "outputId": "0d222cbf-95c4-4566-f74f-1d5c91b668d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/models/model.keras')"
      ],
      "metadata": {
        "id": "2huALznAvh9_"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}